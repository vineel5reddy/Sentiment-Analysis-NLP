{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30170,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://github.com/Shubham-Trivedi/Sentiment-Analysis-on-IMDB-dataset/blob/main/imdb%20logo1.jpg?raw=true)\n\n# Sentiment analysis on Reviews given by viewers on IMDB\n","metadata":{}},{"cell_type":"markdown","source":"\n# The Dataset and The Problem to Solve\n","metadata":{}},{"cell_type":"markdown","source":">In this notebbok, we’ll use an IMDB dataset of 50k movie reviews available on Kaggle. The dataset contains 2 columns (review and sentiment) that will help us identify whether a review is positive or negative.\n>\n>*Problem formulation: Our goal is to find which machine learning model is best suited to predict sentiment (output) given a movie review (input).*","metadata":{}},{"cell_type":"markdown","source":"\n# 1. Importing necessary libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T05:12:57.042572Z","iopub.execute_input":"2022-03-15T05:12:57.042941Z","iopub.status.idle":"2022-03-15T05:12:58.078075Z","shell.execute_reply.started":"2022-03-15T05:12:57.042837Z","shell.execute_reply":"2022-03-15T05:12:58.077304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:12:58.079666Z","iopub.execute_input":"2022-03-15T05:12:58.079931Z","iopub.status.idle":"2022-03-15T05:12:58.095415Z","shell.execute_reply.started":"2022-03-15T05:12:58.079888Z","shell.execute_reply":"2022-03-15T05:12:58.094609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 2. Preparing the data","metadata":{}},{"cell_type":"code","source":"df_review = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf_review","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:12:58.096555Z","iopub.execute_input":"2022-03-15T05:12:58.096947Z","iopub.status.idle":"2022-03-15T05:12:59.55894Z","shell.execute_reply.started":"2022-03-15T05:12:58.096894Z","shell.execute_reply":"2022-03-15T05:12:59.557942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset contains 50000 rows; however, to train our model faster in the following steps, we’re going to take a smaller sample of 10000 rows. This small sample will contain 9000 positive and 1000 negative reviews to make the data imbalanced (so I can teach you undersampling and oversampling techniques in the next step)","metadata":{}},{"cell_type":"code","source":"df_positive = df_review[df_review['sentiment']=='positive'][:9000]\ndf_negative = df_review[df_review['sentiment']=='negative'][:1000]\n\ndf_review_imb = pd.concat([df_positive,df_negative ])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:12:59.56081Z","iopub.execute_input":"2022-03-15T05:12:59.561177Z","iopub.status.idle":"2022-03-15T05:12:59.595996Z","shell.execute_reply.started":"2022-03-15T05:12:59.561143Z","shell.execute_reply":"2022-03-15T05:12:59.595198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with Imbalanced Classes\n\nIn most cases, you’ll have a large amount of data for one class, and much fewer observations for other classes. This is known as imbalanced data because the number of observations per class is not equally distributed.\n\nLet’s take a look at how our df_review_imb dataset is distributed.","metadata":{}},{"cell_type":"code","source":"colors = sns.color_palette('deep')\n\nplt.figure(figsize=(8,4), tight_layout=True)\nplt.bar(x=['Positive', 'Negative'],\n        height=df_review_imb.value_counts(['sentiment']),\n        color=colors[:2])\nplt.title('Sentiment')\nplt.savefig('sentiment.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:12:59.597076Z","iopub.execute_input":"2022-03-15T05:12:59.597275Z","iopub.status.idle":"2022-03-15T05:12:59.867828Z","shell.execute_reply.started":"2022-03-15T05:12:59.597251Z","shell.execute_reply":"2022-03-15T05:12:59.867036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To resample our data we use the imblearn library. You can either undersample positive reviews or oversample negative reviews (you need to choose based on the data you’re working with). In this case, we’ll use the RandomUnderSampler","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state= 0)\ndf_review_bal,df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],df_review_imb['sentiment'])\n\n\ndf_review_bal","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:12:59.869184Z","iopub.execute_input":"2022-03-15T05:12:59.86941Z","iopub.status.idle":"2022-03-15T05:13:00.435807Z","shell.execute_reply.started":"2022-03-15T05:12:59.869383Z","shell.execute_reply":"2022-03-15T05:13:00.434981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can compare the imbalanced and balanced dataset with the following code.","metadata":{}},{"cell_type":"code","source":"print(df_review_imb.value_counts('sentiment'))\nprint(df_review_bal.value_counts('sentiment'))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:00.437227Z","iopub.execute_input":"2022-03-15T05:13:00.43786Z","iopub.status.idle":"2022-03-15T05:13:00.446875Z","shell.execute_reply.started":"2022-03-15T05:13:00.437822Z","shell.execute_reply":"2022-03-15T05:13:00.446044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Splitting data into train and test set\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(df_review_bal,test_size =0.33,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:00.448136Z","iopub.execute_input":"2022-03-15T05:13:00.448371Z","iopub.status.idle":"2022-03-15T05:13:00.456673Z","shell.execute_reply.started":"2022-03-15T05:13:00.448344Z","shell.execute_reply":"2022-03-15T05:13:00.456043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = train['review'], train['sentiment']\ntest_x, test_y = test['review'], test['sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:00.457658Z","iopub.execute_input":"2022-03-15T05:13:00.458032Z","iopub.status.idle":"2022-03-15T05:13:00.466638Z","shell.execute_reply.started":"2022-03-15T05:13:00.457989Z","shell.execute_reply":"2022-03-15T05:13:00.466046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:00.468987Z","iopub.execute_input":"2022-03-15T05:13:00.469377Z","iopub.status.idle":"2022-03-15T05:13:00.484523Z","shell.execute_reply.started":"2022-03-15T05:13:00.469297Z","shell.execute_reply":"2022-03-15T05:13:00.483626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 3.Text Representation (Bag of words)\n","metadata":{}},{"cell_type":"markdown","source":"Classifiers and learning algorithms expect numerical feature vectors rather than raw text documents. This is why we need to turn our movie review text into numerical vectors.\n\nwe’ll use bag of words (BOW) since we care about the frequency of the words in text reviews; however, the order of words is irrelevant. Two common ways to represent bag of words are CountVectorizer and Term Frequency, Inverse Document Frequency (TF-IDF)","metadata":{}},{"cell_type":"markdown","source":"we want to identify unique/representative words for positive reviews and negative reviews, so we’ll choose the TF-IDF. To turn text data into numerical vectors with TF-IDF","metadata":{"execution":{"iopub.status.busy":"2022-03-14T02:31:48.604278Z","iopub.execute_input":"2022-03-14T02:31:48.605113Z","iopub.status.idle":"2022-03-14T02:31:48.612924Z","shell.execute_reply.started":"2022-03-14T02:31:48.605055Z","shell.execute_reply":"2022-03-14T02:31:48.611661Z"}}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ntrain_x_vector = tfidf.fit_transform(train_x)\n# also fit the test_x_vector\ntest_x_vector = tfidf.transform(test_x)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:00.485711Z","iopub.execute_input":"2022-03-15T05:13:00.4865Z","iopub.status.idle":"2022-03-15T05:13:01.008212Z","shell.execute_reply.started":"2022-03-15T05:13:00.486464Z","shell.execute_reply":"2022-03-15T05:13:01.007287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's also transform the test_x_vector, so we can test the accuracy of the model later ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame.sparse.from_spmatrix(train_x_vector,\n                                  index=train_x.index,\n                                  columns=tfidf.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:01.009499Z","iopub.execute_input":"2022-03-15T05:13:01.009706Z","iopub.status.idle":"2022-03-15T05:13:01.898816Z","shell.execute_reply.started":"2022-03-15T05:13:01.009681Z","shell.execute_reply":"2022-03-15T05:13:01.897919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.Model Selection\n\nIn our example, our input (review) and output (sentiment) are clearly identified, so we can say we have labeled input and output data; therefore, we’re dealing with supervised learning.\n\nWe will benchmark the four classification models.\n","metadata":{}},{"cell_type":"markdown","source":"\n##  Support Vector Machine(SVM)\n","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(train_x_vector, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:01.900277Z","iopub.execute_input":"2022-03-15T05:13:01.900661Z","iopub.status.idle":"2022-03-15T05:13:03.618432Z","shell.execute_reply.started":"2022-03-15T05:13:01.900611Z","shell.execute_reply":"2022-03-15T05:13:03.617688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(svc.predict(tfidf.transform(['A good movie'])))\nprint(svc.predict(tfidf.transform(['An excellent movie'])))\nprint(svc.predict(tfidf.transform(['I did not like this movie at all I gave this movie away'])))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:03.619728Z","iopub.execute_input":"2022-03-15T05:13:03.620118Z","iopub.status.idle":"2022-03-15T05:13:03.634539Z","shell.execute_reply.started":"2022-03-15T05:13:03.620073Z","shell.execute_reply":"2022-03-15T05:13:03.633612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Decision Tree\n","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(train_x_vector, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:03.635668Z","iopub.execute_input":"2022-03-15T05:13:03.635979Z","iopub.status.idle":"2022-03-15T05:13:04.034065Z","shell.execute_reply.started":"2022-03-15T05:13:03.635948Z","shell.execute_reply":"2022-03-15T05:13:04.033271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(train_x_vector.toarray(), train_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:04.035312Z","iopub.execute_input":"2022-03-15T05:13:04.035532Z","iopub.status.idle":"2022-03-15T05:13:04.723582Z","shell.execute_reply.started":"2022-03-15T05:13:04.035504Z","shell.execute_reply":"2022-03-15T05:13:04.723037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Logistic Regression\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(train_x_vector,train_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:04.724661Z","iopub.execute_input":"2022-03-15T05:13:04.725007Z","iopub.status.idle":"2022-03-15T05:13:04.936083Z","shell.execute_reply.started":"2022-03-15T05:13:04.724869Z","shell.execute_reply":"2022-03-15T05:13:04.935018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Note: Here notice that we are using review dataframe in vectorized form and sentiment dataframe in normal form it is so that model evaluates the vectorized values of review dataframe and gives out output in normal english not in a vectorized form so that it can be readable to us","metadata":{}},{"cell_type":"markdown","source":"\n# 5. Model Evaluation\n","metadata":{}},{"cell_type":"markdown","source":"## Mean Accuracy","metadata":{}},{"cell_type":"code","source":"print(svc.score(test_x_vector, test_y))\nprint(dec_tree.score(test_x_vector, test_y))\nprint(gnb.score(test_x_vector.toarray(), test_y))\nprint(log_reg.score(test_x_vector, test_y))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:04.937848Z","iopub.execute_input":"2022-03-15T05:13:04.93818Z","iopub.status.idle":"2022-03-15T05:13:06.07331Z","shell.execute_reply.started":"2022-03-15T05:13:04.93814Z","shell.execute_reply":"2022-03-15T05:13:06.071578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM and Logistic Regression perform better than the other two classifiers, with SVM having a slight advantage (84% of accuracy).Thus,we’ll focus only on SVM.","metadata":{}},{"cell_type":"markdown","source":"## F1 Score\n\nF1 Score is the weighted average of Precision and Recall. Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial. Also, F1 takes into account how the data is distributed, so it’s useful when you have data with imbalance classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(test_y,svc.predict(test_x_vector),\n          labels = ['positive','negative'],average=None)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:06.074693Z","iopub.execute_input":"2022-03-15T05:13:06.075031Z","iopub.status.idle":"2022-03-15T05:13:06.820026Z","shell.execute_reply.started":"2022-03-15T05:13:06.07499Z","shell.execute_reply":"2022-03-15T05:13:06.81926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_y,\n                            svc.predict(test_x_vector),\n                            labels = ['positive','negative']))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:06.821415Z","iopub.execute_input":"2022-03-15T05:13:06.821632Z","iopub.status.idle":"2022-03-15T05:13:07.588857Z","shell.execute_reply.started":"2022-03-15T05:13:06.821605Z","shell.execute_reply":"2022-03-15T05:13:07.587991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix\n\nA confusion matrix is a table that allows visualization of the performance of an algorithm. This table typically has two rows and two columns that report the number of false positives, false negatives, true positives, and true negatives","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(test_y,\n                           svc.predict(test_x_vector),\n                           labels = ['positive', 'negative'])\nconf_mat","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:07.590045Z","iopub.execute_input":"2022-03-15T05:13:07.590668Z","iopub.status.idle":"2022-03-15T05:13:08.337013Z","shell.execute_reply.started":"2022-03-15T05:13:07.590631Z","shell.execute_reply":"2022-03-15T05:13:08.336111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 6. Tuning the Model\n","metadata":{}},{"cell_type":"markdown","source":"## GridSearchCV\n\nThis is technique consists of an exhaustive search on specified parameters in order to obtain the optimum values of hyperparameters.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n#set the parameters\nparams = {'C': [1,4,8,16,32], 'kernel' : ['linear','rbf']}\nsvc = SVC()\nsvc_grid = GridSearchCV(svc,params, cv = 5)\nsvc_grid.fit(train_x_vector, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:13:08.338079Z","iopub.execute_input":"2022-03-15T05:13:08.338275Z","iopub.status.idle":"2022-03-15T05:14:20.23002Z","shell.execute_reply.started":"2022-03-15T05:13:08.338252Z","shell.execute_reply":"2022-03-15T05:14:20.229057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(svc_grid.best_params_)\nprint(svc_grid.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:14:20.231195Z","iopub.execute_input":"2022-03-15T05:14:20.231409Z","iopub.status.idle":"2022-03-15T05:14:20.238633Z","shell.execute_reply.started":"2022-03-15T05:14:20.231381Z","shell.execute_reply":"2022-03-15T05:14:20.237745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So Above obtained paramters and kernel are the best for our model to obtain desired results as accurate as possible","metadata":{}}]}